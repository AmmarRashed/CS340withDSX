{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "name": "python"
        }, 
        "kernelspec": {
            "name": "python2", 
            "language": "python", 
            "display_name": "Python 2 with Spark 1.6"
        }
    }, 
    "cells": [
        {
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "source": "name = 'keystone'\npathToSampleFile = \"swift://CS340.\" + name + \"/NetflowData.txt\"\nrdd = sc.textFile(pathToSampleFile)", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 4, 
                    "data": {
                        "text/plain": "PythonRDD[2] at RDD at PythonRDD.scala:43"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.filter(lambda line: \"Python\" in line)", 
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "0"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.filter(lambda line: \"Python\" in line)\\\n    .count()", 
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "[u'10.1.0.2,16.2.3.7,12,20K,http',\n u'18.6.7.1,12.4.0.3,16,24K,http',\n u'13.9.4.3,11.6.8.2,15,20K,http',\n u'15.2.2.9,17.1.2.1,19,40K,http',\n u'12.4.3.8,14.8.7.4,26,58K,http',\n u'10.5.1.3,13.0.0.1,27,100K,ftp',\n u'11.1.0.6,10.3.4.5,32,300K,ftp',\n u'19.7.1.2,16.5.5.8,18,80K,ftp',\n u'10.110.0.2,16.12.3.7,12,20K,http',\n u'182.6.7.1,12.4.0.13,16,124K,http',\n u'163.9.4.3,11.6.8.2,15,20K,http',\n u'15.2.12.9,17.17.2.1,19,140K,http',\n u'12.4.3.8,14.8.71.42,26,58K,http',\n u'10.45.1.3,13.0.0.1,27,200K,http',\n u'11.1.20.6,10.3.4.5,32,300K,ftp',\n u'19.47.1.2,16.5.5.8,18,180K,http',\n u'10.1.0.2,16.2.3.7,12,20K,http',\n u'18.6.71.1,12.4.0.3,16,24K,http',\n u'13.9.4.3,21.6.8.2,15,20K,http',\n u'15.22.2.9,17.1.2.1,19,40K,http',\n u'12.4.3.8,24.8.7.4,26,58K,ftp',\n u'10.55.1.3,13.1.0.1,27,100K,ftp',\n u'11.1.0.6,10.23.4.5,32,30K,ftp',\n u'191.7.1.2,16.15.5.8,18,80K,ftp',\n u'110.110.0.2,16.22.31.7,12,120K,ftp',\n u'182.6.72.1,12.14.0.13,16,124K,http',\n u'163.9.4.3,11.6.8.2,15,20K,http',\n u'25.2.12.9,17.17.2.1,19,140K,http',\n u'12.4.3.81,14.8.7.42,26,58K,http',\n u'10.45.1.3,13.0.0.1,27,280K,http',\n u'11.1.20.16,10.3.4.5,32,35K,ftp',\n u'19.47.1.2,16.5.5.8,18,180K,http']"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.collect()", 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<center><h1>Task Execution</h1></center>\n![Image of Task Execution](http://training.databricks.com/databricks_guide/gentle_introduction/videoss_logo.png)", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "![Image of Task Execution](http://training.databricks.com/databricks_guide/gentle_introduction/spark_cluster_tasks.png)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 7, 
                    "data": {
                        "text/plain": "[u'10.1.0.2,16.2.3.7,12,20K,http', u'18.6.7.1,12.4.0.3,16,24K,http']"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.take(2)", 
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 8, 
                    "data": {
                        "text/plain": "PythonRDD[5] at RDD at PythonRDD.scala:43"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.filter(lambda line: line.endswith(\"http\"))", 
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 9, 
                    "data": {
                        "text/plain": "1810"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.filter(lambda line: line.endswith(\"http\"))\\\n    .map(lambda line: int(line.split(\",\")[-2][:-1]))\\\n    .reduce(lambda x,y: x+y)", 
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<h1>Spark Operations: Transformations and Actions</h1>\n<p>\n<h2>Transformations: correspond to \"instructions\" that massage the data.</h2>\n<p>\n<h2>Actions: return actual results to the user.</h2>", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<strong>By calling transformations and actions, we at the Driver dictate \nexecutors living in Workers to execute tasks for our jobs. </strong>\n<p>\n![](https://github.com/ahmetbulut/CS340withDSX/blob/master/SparkWorkflow.png?raw=true)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [], 
            "source": "def extractPayLoad(line):\n    line = line.split(\",\")\n    payload = line[-2]\n    stripped = payload[:-1] \n    return int(stripped)", 
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 11, 
                    "data": {
                        "text/plain": "1810"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.filter(lambda line: line.endswith(\"http\"))\\\n    .map(extractPayLoad)\\\n    .reduce(lambda x,y: x+y)", 
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 12, 
                    "data": {
                        "text/plain": "MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)", 
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<b>There are multiple ways to store an RDD.</b>\n<ol>\n<li>DISK_ONLY</li>\n<li>MEMORY_AND_DISK</li>\n<li>MEMORY_ONLY</li>\n<li>...</li>\n</ol>", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<h2>Persistence</h2>", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 29, 
                    "data": {
                        "text/plain": "['DISK_ONLY',\n 'DISK_ONLY_2',\n 'MEMORY_AND_DISK',\n 'MEMORY_AND_DISK_2',\n 'MEMORY_AND_DISK_SER',\n 'MEMORY_AND_DISK_SER_2',\n 'MEMORY_ONLY',\n 'MEMORY_ONLY_2',\n 'MEMORY_ONLY_SER',\n 'MEMORY_ONLY_SER_2',\n 'OFF_HEAP',\n '__class__',\n '__delattr__',\n '__dict__',\n '__doc__',\n '__format__',\n '__getattribute__',\n '__hash__',\n '__init__',\n '__module__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__']"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "dir(pyspark.StorageLevel)", 
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "source": "<h2>Magic Commands</h2>\nThere are numerous built-in magic commands. You can run cells with bash \nin a subprocess as demonstrated in the following cell. \n\nSee <a href=\"https://ipython.org/ipython-doc/3/interactive/magics.html\">built-in magic commands</a>", 
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "markdown"
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Mon Feb 27 05:53:12 CST 2017\n"
                }
            ], 
            "source": "%%bash\ndate", 
            "execution_count": 39, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "outputs": [], 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0
}